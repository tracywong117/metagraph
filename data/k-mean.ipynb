{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binary_file(file):\n",
    "    \"\"\"\n",
    "    Load a numpy array from a binary file\n",
    "    \"\"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        packed_data = f.read()\n",
    "    data = np.frombuffer(packed_data, dtype=np.uint64)\n",
    "    # return np.unpackbits(data, axis=-1, bitorder='little')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_binary = load_binary_file('output_set.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60801408\n",
      "[ 9516115917310651001  9516115917312745613  9516115917717502581 ...\n",
      " 17725741141956831395 17725741141956835459 17725741142021845635]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_binary))\n",
    "print(all_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(a, b):\n",
    "    \"\"\"Calculate the Hamming distance using bit_count() (Python 3.10+).\"\"\"\n",
    "    return (a ^ b).bit_count()\n",
    "\n",
    "def hamming_distances_vectorized(point, lst):\n",
    "    # Convert inputs to NumPy arrays\n",
    "    # point = np.uint64(point)\n",
    "    # lst = np.array(lst, dtype=np.uint64)\n",
    "    \n",
    "    # XOR the point with the list\n",
    "    xor_result = point ^ lst  # XOR result is still uint64\n",
    "    \n",
    "    # Convert uint64 to uint8 view (8 bytes per uint64)\n",
    "    xor_bytes = xor_result.view(np.uint8)  # Interpret each uint64 as 8 uint8 values\n",
    "    \n",
    "    # Unpack bits and count the number of 1s for each uint64\n",
    "    unpacked_bits = np.unpackbits(xor_bytes, axis=0).reshape(len(lst), 64)\n",
    "    distances = unpacked_bits.sum(axis=1)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def compute_centroid(cluster):\n",
    "    \"\"\"\n",
    "    Compute the centroid (bitwise majority) of a cluster of binary codes.\n",
    "    \n",
    "    Args:\n",
    "        cluster (list of int): List of 64-bit binary codes as integers.\n",
    "        \n",
    "    Returns:\n",
    "        int: Centroid as a 64-bit binary code.\n",
    "    \"\"\"\n",
    "    if not cluster:\n",
    "        return None\n",
    "    \n",
    "    # Convert cluster to an array of bits\n",
    "    cluster_bits = np.array([list(map(int, bin(code)[2:].zfill(64))) for code in cluster])\n",
    "    \n",
    "    # Compute bitwise majority\n",
    "    majority_bits = (cluster_bits.sum(axis=0) >= (len(cluster) / 2)).astype(int)\n",
    "    \n",
    "    # Convert majority bits back to integer\n",
    "    centroid = int(\"\".join(map(str, majority_bits)), 2)\n",
    "    return centroid\n",
    "\n",
    "def kmeans_hamming(dataset, k, max_iters=100):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering using Hamming distance for 64-bit binary codes.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list of int): List of 64-bit binary codes as integers.\n",
    "        k (int): Number of clusters.\n",
    "        max_iters (int): Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        list: Cluster assignments for each data point.\n",
    "        list: Final centroids for each cluster.\n",
    "    \"\"\"\n",
    "    n = len(dataset)\n",
    "    \n",
    "    # Step 1: Initialize centroids (randomly select k points from the dataset)\n",
    "    centroids = np.random.choice(dataset, k, replace=False)\n",
    "    \n",
    "    # Step 2: Iterate until convergence or max iterations\n",
    "    for iteration in range(max_iters):\n",
    "        print(f\"Iteration {iteration + 1}\")\n",
    "        # Assignment step: Assign each point to the nearest centroid\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for point in tqdm(dataset):\n",
    "            # distances = [hamming_distance(point, centroid) for centroid in centroids]\n",
    "            distances = hamming_distances_vectorized(point, np.array(centroids))\n",
    "            cluster_idx = np.argmin(distances)\n",
    "            clusters[cluster_idx].append(point)\n",
    "        \n",
    "        # Update step: Compute new centroids\n",
    "        new_centroids = []\n",
    "        for cluster in clusters:\n",
    "            if cluster:\n",
    "                new_centroids.append(compute_centroid(cluster))\n",
    "            else:\n",
    "                # If a cluster is empty, reinitialize its centroid randomly\n",
    "                new_centroids.append(np.random.choice(dataset))\n",
    "        \n",
    "        # Check for convergence (centroids do not change)\n",
    "        if np.array_equal(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    # Final assignment\n",
    "    cluster_assignments = [None] * n\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for point in cluster:\n",
    "            cluster_assignments[dataset.index(point)] = cluster_idx\n",
    "    \n",
    "    return cluster_assignments, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 66922/60801408 [00:05<1:21:37, 12401.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform K-means clustering\u001b[39;00m\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m  \u001b[38;5;66;03m# Number of clusters\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cluster_assignments, centroids \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_hamming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 87\u001b[0m, in \u001b[0;36mkmeans_hamming\u001b[0;34m(dataset, k, max_iters)\u001b[0m\n\u001b[1;32m     84\u001b[0m clusters \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k)]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# distances = [hamming_distance(point, centroid) for centroid in centroids]\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43mhamming_distances_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     cluster_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(distances)\n\u001b[1;32m     89\u001b[0m     clusters[cluster_idx]\u001b[38;5;241m.\u001b[39mappend(point)\n",
      "Cell \u001b[0;32mIn[64], line 35\u001b[0m, in \u001b[0;36mhamming_distances_vectorized\u001b[0;34m(point, lst)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Unpack bits and count the number of 1s for each uint64\u001b[39;00m\n\u001b[1;32m     34\u001b[0m unpacked_bits \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munpackbits(xor_bytes, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(lst), \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43munpacked_bits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distances\n",
      "File \u001b[0;32m~/anaconda3/envs/mgdb-py38/lib/python3.8/site-packages/numpy/core/_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform K-means clustering\n",
    "k = 2000  # Number of clusters\n",
    "cluster_assignments, centroids = kmeans_hamming(all_binary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def hamming_distance(a, b):\n",
    "    \"\"\"Calculate the Hamming distance using bit_count() (Python 3.10+).\"\"\"\n",
    "    return (a ^ b).bit_count()\n",
    "\n",
    "\n",
    "def compute_centroid(cluster):\n",
    "    \"\"\"\n",
    "    Compute the centroid (bitwise majority) of a cluster of binary codes.\n",
    "    \n",
    "    Args:\n",
    "        cluster (list of int): List of 64-bit binary codes as integers.\n",
    "        \n",
    "    Returns:\n",
    "        int: Centroid as a 64-bit binary code.\n",
    "    \"\"\"\n",
    "    if not cluster:\n",
    "        return None\n",
    "    \n",
    "    # Convert cluster to an array of bits\n",
    "    cluster_bits = np.array([list(map(int, bin(code)[2:].zfill(64))) for code in cluster])\n",
    "    \n",
    "    # Compute bitwise majority\n",
    "    majority_bits = (cluster_bits.sum(axis=0) >= (len(cluster) / 2)).astype(int)\n",
    "    \n",
    "    # Convert majority bits back to integer\n",
    "    centroid = int(\"\".join(map(str, majority_bits)), 2)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "def compute_hamming_distances(args):\n",
    "    \"\"\"\n",
    "    Compute the Hamming distances between a point and all centroids.\n",
    "    \n",
    "    Args:\n",
    "        args (tuple): (point, centroids)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (point, index of closest centroid, distance to closest centroid)\n",
    "    \"\"\"\n",
    "    point, centroids = args\n",
    "    distances = [(point ^ centroid).bit_count() for centroid in centroids]\n",
    "    cluster_idx = np.argmin(distances)\n",
    "    return point, cluster_idx\n",
    "\n",
    "\n",
    "def kmeans_hamming(dataset, k, max_iters=100):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering using Hamming distance for 64-bit binary codes.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list of int): List of 64-bit binary codes as integers.\n",
    "        k (int): Number of clusters.\n",
    "        max_iters (int): Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        list: Cluster assignments for each data point.\n",
    "        list: Final centroids for each cluster.\n",
    "    \"\"\"\n",
    "    n = len(dataset)\n",
    "    \n",
    "    # Step 1: Initialize centroids (randomly select k points from the dataset)\n",
    "    centroids = np.random.choice(dataset, k, replace=False)\n",
    "    cluster_assignments = [None] * n\n",
    "    \n",
    "    # Step 2: Iterate until convergence or max iterations\n",
    "    for iteration in range(max_iters):\n",
    "        print(f\"Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Assignment step: Assign each point to the nearest centroid\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        \n",
    "        # Use multiprocessing to compute distances in parallel\n",
    "        with Pool(cpu_count()) as pool:\n",
    "            results = list(tqdm(pool.imap(compute_hamming_distances, [(point, centroids) for point in dataset]), total=n))\n",
    "        \n",
    "        # Update cluster assignments based on results\n",
    "        for point, cluster_idx in results:\n",
    "            clusters[cluster_idx].append(point)\n",
    "        \n",
    "        # Update step: Compute new centroids\n",
    "        new_centroids = []\n",
    "        for cluster in clusters:\n",
    "            if cluster:\n",
    "                new_centroids.append(compute_centroid(cluster))\n",
    "            else:\n",
    "                # If a cluster is empty, reinitialize its centroid randomly\n",
    "                new_centroids.append(np.random.choice(dataset))\n",
    "        \n",
    "        # Check for convergence (centroids do not change)\n",
    "        if np.array_equal(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    # Final assignment\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for point in cluster:\n",
    "            cluster_assignments[dataset.index(point)] = cluster_idx\n",
    "    \n",
    "    return cluster_assignments, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 73681/60801408 [00:12<2:55:45, 5758.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/mgdb-py38/lib/python3.8/multiprocessing/pool.py:851\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform K-means clustering\u001b[39;00m\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m  \u001b[38;5;66;03m# Number of clusters\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cluster_assignments, centroids \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_hamming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 79\u001b[0m, in \u001b[0;36mkmeans_hamming\u001b[0;34m(dataset, k, max_iters)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Use multiprocessing to compute distances in parallel\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(cpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 79\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_hamming_distances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Update cluster assignments based on results\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point, cluster_idx \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/anaconda3/envs/mgdb-py38/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mgdb-py38/lib/python3.8/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/envs/mgdb-py38/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform K-means clustering\n",
    "k = 20000  # Number of clusters\n",
    "cluster_assignments, centroids = kmeans_hamming(all_binary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Cluster Assignments:\", cluster_assignments)\n",
    "print(\"Centroids:\")\n",
    "for centroid in centroids:\n",
    "    print(bin(centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HammingDistance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m target \u001b[38;5;241m=\u001b[39m tensor([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m      3\u001b[0m preds \u001b[38;5;241m=\u001b[39m tensor([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m----> 4\u001b[0m hamming_distance \u001b[38;5;241m=\u001b[39m \u001b[43mHammingDistance\u001b[49m(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m hamming_distance(preds, target)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HammingDistance' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import tensor\n",
    "target = tensor([[0, 1], [1, 1]])\n",
    "preds = tensor([[0, 1], [0, 1]])\n",
    "hamming_distance = HammingDistance(task=\"multilabel\", num_labels=2)\n",
    "hamming_distance(preds, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hamming_distance_numpy_optimized(point, lst):\n",
    "    # Convert inputs to NumPy arrays\n",
    "    point_array = np.uint64(point)\n",
    "    lst_array = np.array(lst, dtype=np.uint64)\n",
    "    \n",
    "    # XOR the point with the list\n",
    "    xor_result = point_array ^ lst_array  # XOR result is still uint64\n",
    "    \n",
    "    # Convert uint64 to uint8 view (8 bytes per uint64)\n",
    "    xor_bytes = xor_result.view(np.uint8)  # Interpret each uint64 as 8 uint8 values\n",
    "    \n",
    "    # Unpack bits and count the number of 1s for each uint64\n",
    "    unpacked_bits = np.unpackbits(xor_bytes, axis=0).reshape(len(lst), 64)\n",
    "    distances = unpacked_bits.sum(axis=1)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Example usage\n",
    "point = 0b101010  # uint64 point\n",
    "lst = [0b111000, 0b101010, 0b000111]  # List of uint64 values\n",
    "distances = hamming_distance_numpy_optimized(point, lst)\n",
    "print(distances)  # Output: [3, 0, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuebing/anaconda3/envs/mgdb-py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels: [0 0 0 1 1 1]\n",
      "Centroids: [[ 1.          2.6       ]\n",
      " [10.          1.76190476]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "\n",
    "# Generate some sample data\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "# Initialize and fit Mini-Batch K-Means\n",
    "n_clusters = 2  # Number of clusters\n",
    "mbk = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=3)\n",
    "mbk.fit(X)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "labels = mbk.predict(X)\n",
    "\n",
    "# Print the labels\n",
    "print(\"Cluster labels:\", labels)\n",
    "\n",
    "# Print the cluster centroids\n",
    "print(\"Centroids:\", mbk.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import random\n",
    "\n",
    "def hamming_distance(a, b):\n",
    "    \"\"\"Calculate the Hamming distance using bit_count() (Python 3.10+).\"\"\"\n",
    "    return (a ^ b).bit_count()\n",
    "\n",
    "def hamming_distances_vectorized(point, lst):\n",
    "    # Convert inputs to NumPy arrays\n",
    "    # point = np.uint64(point)\n",
    "    # lst = np.array(lst, dtype=np.uint64)\n",
    "    \n",
    "    # XOR the point with the list\n",
    "    xor_result = point ^ lst  # XOR result is still uint64\n",
    "    \n",
    "    # Convert uint64 to uint8 view (8 bytes per uint64)\n",
    "    xor_bytes = xor_result.view(np.uint8)  # Interpret each uint64 as 8 uint8 values\n",
    "    \n",
    "    # Unpack bits and count the number of 1s for each uint64\n",
    "    unpacked_bits = np.unpackbits(xor_bytes, axis=0).reshape(len(lst), 64)\n",
    "    distances = unpacked_bits.sum(axis=1)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "\n",
    "def compute_centroid(cluster):\n",
    "    \"\"\"\n",
    "    Compute the centroid (bitwise majority) of a cluster of binary codes.\n",
    "    \n",
    "    Args:\n",
    "        cluster (list of int): List of 64-bit binary codes as integers.\n",
    "        \n",
    "    Returns:\n",
    "        int: Centroid as a 64-bit binary code.\n",
    "    \"\"\"\n",
    "    if not cluster:\n",
    "        return None\n",
    "    \n",
    "    # Convert cluster to an array of bits\n",
    "    cluster_bits = np.array([list(map(int, bin(code)[2:].zfill(64))) for code in cluster])\n",
    "    \n",
    "    # Compute bitwise majority\n",
    "    majority_bits = (cluster_bits.sum(axis=0) >= (len(cluster) / 2)).astype(int)\n",
    "    \n",
    "    # Convert majority bits back to integer\n",
    "    centroid = int(\"\".join(map(str, majority_bits)), 2)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "def compute_hamming_distances(args):\n",
    "    \"\"\"\n",
    "    Compute the Hamming distances between a point and all centroids.\n",
    "    \n",
    "    Args:\n",
    "        args (tuple): (point, centroids)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (point, index of closest centroid, distance to closest centroid)\n",
    "    \"\"\"\n",
    "    point, centroids = args\n",
    "    # distances = [(point ^ centroid).bit_count() for centroid in centroids]\n",
    "    distances = hamming_distances_vectorized(point, np.array(centroids))\n",
    "    cluster_idx = np.argmin(distances)\n",
    "    return point, cluster_idx\n",
    "\n",
    "\n",
    "def mini_batch_kmeans_hamming(dataset, k, batch_size=1000, max_iters=10):\n",
    "    \"\"\"\n",
    "    Perform Mini-Batch K-means clustering using Hamming distance for 64-bit binary codes.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list of int): List of 64-bit binary codes as integers.\n",
    "        k (int): Number of clusters.\n",
    "        batch_size (int): Size of each mini-batch.\n",
    "        max_iters (int): Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        list: Cluster assignments for each data point.\n",
    "        list: Final centroids for each cluster.\n",
    "    \"\"\"\n",
    "    n = len(dataset)\n",
    "    \n",
    "    # Step 1: Initialize centroids (randomly select k points from the dataset)\n",
    "    centroids = np.random.choice(dataset, k, replace=False)\n",
    "    \n",
    "    # Initialize cluster assignments\n",
    "    cluster_assignments = [None] * n\n",
    "    cluster_counts = [0] * k  # Track number of points assigned to each cluster\n",
    "    \n",
    "    # Step 2: Iterate until convergence or max iterations\n",
    "    for iteration in range(max_iters):\n",
    "        print(f\"Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Step 2.1: Sample a mini-batch from the dataset\n",
    "        # mini_batch = random.sample(dataset, batch_size)\n",
    "        # Step 2.1: Sample a mini-batch from the dataset\n",
    "        mini_batch = random.sample(list(dataset), batch_size)\n",
    "        \n",
    "        # Step 2.2: Assign points in the mini-batch to the nearest centroid\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        \n",
    "        # Use multiprocessing to compute distances in parallel\n",
    "        with Pool(cpu_count()) as pool:\n",
    "            results = list(pool.imap(compute_hamming_distances, [(point, centroids) for point in mini_batch]))\n",
    "        \n",
    "        # Update cluster assignments and counts based on mini-batch results\n",
    "        for point, cluster_idx in results:\n",
    "            clusters[cluster_idx].append(point)\n",
    "            cluster_counts[cluster_idx] += 1\n",
    "        \n",
    "        # Step 2.3: Update centroids incrementally using the mini-batch\n",
    "        new_centroids = centroids.copy()\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            if cluster:\n",
    "                cluster_centroid = compute_centroid(cluster)\n",
    "                \n",
    "                # Incrementally update centroid using a weighted average\n",
    "                if cluster_counts[cluster_idx] > 0:\n",
    "                    new_centroids[cluster_idx] = (\n",
    "                        (cluster_counts[cluster_idx] - len(cluster)) * centroids[cluster_idx] + cluster_centroid\n",
    "                    ) // cluster_counts[cluster_idx]\n",
    "        \n",
    "        # Check for convergence (if centroids do not change)\n",
    "        if np.array_equal(centroids, new_centroids):\n",
    "            print(\"Convergence reached.\")\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    # Final assignment: Assign all points in the dataset to the nearest centroid\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        final_results = list(tqdm(pool.imap(compute_hamming_distances, [(point, centroids) for point in dataset]), total=n))\n",
    "    \n",
    "    for i, (point, cluster_idx) in enumerate(final_results):\n",
    "        cluster_assignments[i] = cluster_idx  # Assign cluster index directly by position\n",
    "        \n",
    "    # Save the cluster assignments and centroids\n",
    "    np.save(\"cluster_assignments.npy\", cluster_assignments)\n",
    "    np.save(\"centroids.npy\", centroids)\n",
    "    \n",
    "    return cluster_assignments, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 85\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n",
      "Iteration 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60801408/60801408 [3:01:21<00:00, 5587.49it/s]  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Perform K-means clustering\n",
    "k = 20000  # Number of clusters\n",
    "cluster_assignments, centroids = mini_batch_kmeans_hamming(all_binary, k, batch_size=10000, max_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 330,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 330,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 330,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 330,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 330,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 1743,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 1646,\n",
       " 1646,\n",
       " 2,\n",
       " 2,\n",
       " 709,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1621,\n",
       " 1621,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1621,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1621,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1621,\n",
       " 2,\n",
       " 1621,\n",
       " 1621,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 709,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1930,\n",
       " 2,\n",
       " 330,\n",
       " 2,\n",
       " 1516,\n",
       " 312,\n",
       " 2,\n",
       " 1621,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 330,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 1621,\n",
       " 1621,\n",
       " 1621,\n",
       " 1621,\n",
       " 1621,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 330,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1621,\n",
       " 1621,\n",
       " 1621,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 1621,\n",
       " 1446,\n",
       " 1621,\n",
       " 2,\n",
       " 312,\n",
       " 330,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 445,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 1646,\n",
       " 312,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1446,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1646,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 330,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1646,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 1743,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 22,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 330,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 312,\n",
       " 2,\n",
       " 1646,\n",
       " 2,\n",
       " 312,\n",
       " 312,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 709,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 704902902040065408, 1189589327514942464,  288381041079352576, ...,\n",
       "       4758102010793744384, 5709797372109057024, 6344124661326496768],\n",
       "      dtype=uint64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Hamming distance to centroids: 21.129975\n"
     ]
    }
   ],
   "source": [
    "# Check average Hamming distance to centroids\n",
    "avg_distances = []\n",
    "for i, point in enumerate(all_binary[:40000]):\n",
    "    centroid = centroids[cluster_assignments[i]]\n",
    "    avg_distances.append(hamming_distance(point, centroid))\n",
    "    \n",
    "print(\"Average Hamming distance to centroids:\", np.mean(avg_distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Hamming distance to centroids: 13\n"
     ]
    }
   ],
   "source": [
    "# Check minimum Hamming distance to centroids\n",
    "min_distances = []\n",
    "for i, point in enumerate(all_binary[:40000]):\n",
    "    centroid = centroids[cluster_assignments[i]]\n",
    "    min_distances.append(hamming_distance(point, centroid))\n",
    "\n",
    "print(\"Minimum Hamming distance to centroids:\", np.min(min_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Hamming distances between centroids\n",
    "pairwise_distances = []\n",
    "for i, centroid1 in enumerate(centroids):\n",
    "    for j, centroid2 in enumerate(centroids):\n",
    "        if i < j:\n",
    "            pairwise_distances.append(hamming_distance(centroid1, centroid2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgdb-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
